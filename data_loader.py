import pandas as pd
import os
import streamlit as st
import requests
from io import BytesIO

@st.cache_data
def get_krx_list():
    file_path = 'krx_list.csv'
    
    if os.path.exists(file_path):
        return pd.read_csv(file_path, dtype={'ticker': str})
    
    try:
        url = 'https://kind.krx.co.kr/corpgeneral/corpList.do'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'
        }
        
        # 1. ì½”ìŠ¤ë‹¥/ì½”ìŠ¤í”¼ ë°ì´í„°ë¥¼ ê°ê° ê°€ì ¸ì˜¤ê¸°
        # marketType: stockMkt(ìœ ê°€ì¦ê¶Œ), kosdaqMkt(ì½”ìŠ¤ë‹¥)
        dfs = []
        for m_type in ["stockMkt", "kosdaqMkt"]:
            params = {
                'method': 'download',
                'marketType': m_type
            }
            # verify=FalseëŠ” SSL ì¸ì¦ì„œ ì—ëŸ¬ ë°©ì§€ìš© (í•„ìš”ì‹œ)
            res = requests.get(url, params=params, headers=headers, timeout=10)
            
            # [í•µì‹¬] read_htmlì— StringIO ëŒ€ì‹  BytesIOë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ì½”ë”© ë¬¸ì œë¥¼ ë°©ì§€í•˜ê³ 
            # flavor='bs4' ë˜ëŠ” 'lxml'ì„ ëª…ì‹œí•©ë‹ˆë‹¤.
            try:
                # KRX í…Œì´ë¸” ë°ì´í„°ëŠ” 'euc-kr'ì¸ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
                temp_df = pd.read_html(BytesIO(res.content), encoding='euc-kr')[0]
                
                # í‹°ì»¤ í¬ë§· ì •ë¦¬
                suffix = '.KS' if m_type == "stockMkt" else '.KQ'
                temp_df['ticker'] = temp_df['ì¢…ëª©ì½”ë“œ'].astype(str).str.zfill(6) + suffix
                dfs.append(temp_df)
            except Exception as inner_e:
                print(f"{m_type} ë¡œë“œ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜: {inner_e}")
                continue

        if not dfs:
            raise ValueError("ë°ì´í„°ë¥¼ í•˜ë‚˜ë„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (No tables found)")

        # 2. í†µí•© ë° ì •ë¦¬
        df = pd.concat(dfs, ignore_index=True)
        df = df[['íšŒì‚¬ëª…', 'ticker']].copy()
        df.columns = ['name', 'ticker']
        df['display'] = "ğŸ‡°ğŸ‡· " + df['name'] + " (" + df['ticker'] + ")"
        
        # 3. CSV ì €ì¥
        df.to_csv(file_path, index=False, encoding='utf-8-sig')
        return df
        
    except Exception as e:
        st.error(f"í•œêµ­ ì£¼ì‹ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ìµœì¢… ì‹¤íŒ¨: {e}")
        # ìµœí›„ì˜ ìˆ˜ë‹¨: ì•±ì´ ë©ˆì¶”ì§€ ì•Šë„ë¡ ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
        return pd.DataFrame({
            'name': ['ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤'],
            'ticker': ['005930.KS', '000660.KS'],
            'display': ['ğŸ‡°ğŸ‡· ì‚¼ì„±ì „ì (005930.KS)', 'ğŸ‡°ğŸ‡· SKí•˜ì´ë‹‰ìŠ¤ (000660.KS)']
        })

@st.cache_data
def get_us_list():
    """SEC ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ë¯¸êµ­ ì£¼ì‹ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ê¸°ì¡´ ìœ ì§€)"""
    file_path = 'us_stocks.csv'
    if os.path.exists(file_path):
        df = pd.read_csv(file_path)
        if 'display' in df.columns: return df
    
    import requests
    try:
        url = "https://www.sec.gov/files/company_tickers.json"
        headers = {"User-Agent": "MyStockApp contact@example.com"}
        res = requests.get(url, headers=headers, timeout=10)
        data = res.json()
        raw_df = pd.DataFrame.from_dict(data, orient="index")
        df = raw_df[['title', 'ticker']].copy()
        df.columns = ['name', 'ticker']
        df['display'] = "ğŸ‡ºğŸ‡¸ " + df['ticker'] + " - " + df['name']
        df.to_csv(file_path, index=False, encoding='utf-8-sig')
        return df
    except:
        return pd.DataFrame(columns=['name', 'ticker', 'display'])